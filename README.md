# ML & Pytorch

## 参考： python机器学习基础教程

---

`ML_1.py` 基本库的简单用法(numpy，SciPy，matplotlib，pandas)  

`ML_2.py` 基本ML使用(scikit-learn k邻近算法)

`ML_3.py` 基本ML使用(scikit-learn 线性回归)

`ML_4.py` 基本ML使用(scikit-learn 决策树)

---

`network.py` 使用numpy，SciPy搭建简单的三层神经网络

`regression.py` 使用pyTorch搭建简单的三层神经网络,用于回归

`classificiation.py` 使用pyTorch搭建简单的三层神经网络,用于分类

`cnn.py` 使用pyTorch搭建卷积神经网络,用于手写数字识别

---

# 《动手学深度学习》(PyTorch版)

`https://tangshusen.me/Dive-into-DL-PyTorch/#/`

---

`tensorTest.py` 预备知识，Tensor基本使用

`tensorTest2.py` PyTorch 原始方式实现线性回归

`tensorTest3.py` PyTorch 正常方式实现线性回归

`torchVisTest.py` PyTorch 原始方式实现softmax回归

`torchVisTest2.py` PyTorch 正常方式实现softmax回归

`mlpTest.py` PyTorch 正常方式实现MLP

`kaggleTest.py` Kaggle 房价预测

---

`cnnTest1.py` 通过数据学习核数组



---

## 深度学习基础

### 2. 预备知识

`torch.Tensor`是存储和变换数据的主要工具。Tensor和NumPy的多维数组非常类似。然而，Tensor提供GPU计算和自动求梯度等更多功能，这些使Tensor更加适合深度学习。

"tensor"这个单词一般可译作“张量”，张量可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。

### 3.1 线性回归

当模型和损失函数形式较为简单时，误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。

然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。

推荐文章： `https://blog.csdn.net/lilyth_lilyth/article/details/8973972`

和大多数深度学习模型一样，对于线性回归这样一种单层神经网络，它的基本要素包括模型、训练数据、损失函数和优化算法。

既可以用神经网络图表示线性回归，又可以用矢量计算表示该模型。

`torch.utils.data`模块提供了有关数据处理的工具，

`torch.nn`模块定义了大量神经网络的层，

`torch.nn.init`模块定义了各种初始化方法，

`torch.optim`模块提供了很多常用的优化算法。

### 3.4 softmax回归

虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2、3等离散值之一，但这种连续值到离散值的转化通常会影响到分类质量。因此我们一般使用更加适合离散值输出的模型来解决分类问题。

`softmax`回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。

![softmax](math.svg)

交叉熵适合衡量两个概率分布的差异。

`torchvision`包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。

`torchvision.datasets`: 一些加载数据的函数及常用的数据集接口；

`torchvision.models`: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；

`torchvision.transforms`: 常用的图片变换，例如裁剪、旋转等；

`torchvision.utils`: 其他的一些有用的方法。


### 3.8 多层感知机（multilayer perceptron，MLP）

多层感知机在输出层与输入层之间加入了一个或多个全连接隐藏层，并通过激活函数对隐藏层输出进行变换。

激活函数:

`ReLU`函数只保留正数元素，并将负数元素清零。

`sigmoid`函数。当输入接近0时，sigmoid函数接近线性变换。

`tanh`（双曲正切）函数可以将元素的值变换到-1和1之间

由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。机器学习模型应关注降低泛化误差。

### `TODO` 3.12 权重衰减  3.13 丢弃法  3.14 正向传播、反向传播和计算图  3.15 数值稳定性和模型初始化

## 深度学习计算

虽然Sequential等类可以使模型构造更加简单，但直接继承Module类可以极大地拓展模型构造的灵活性。

可以通过Module类自定义神经网络中的层，从而可以被重复调用。

通过save函数和load函数可以很方便地读写Tensor。

通过save函数和load_state_dict函数可以很方便地读写模型的参数。

PyTorch可以指定用来存储和计算的设备，如使用内存的CPU或者使用显存的GPU。在默认情况下，PyTorch会将数据创建在内存，然后利用CPU来计算。

PyTorch要求计算的所有输入数据都在内存或同一块显卡的显存上。

## 卷积神经网络

虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。

在深度学习中核数组都是学出来的，卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。

通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。

二维卷积层的核心计算是二维互相关运算。在最简单的形式下，它对二维输入数据和卷积核做互相关运算然后加上偏差。

卷积层的两个超参数，即填充（padding）和步幅（stride）

填充可以增加输出的高和宽。这常用来使输出与输入具有相同的高和宽。步幅可以减小输出的高和宽，

输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×11×1卷积层的作用与全连接层等价。

![1x1 conv](1x1.svg)

池化（pooling）层，它的提出是为了缓解卷积层对位置的过度敏感性。

![LeNet](lenet.png)

> LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。

> 卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用5×55×5的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为2×22×2，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。

> 卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。

![AlexNet](alexnet.png)

推荐文章： `https://blog.csdn.net/weixin_42111770/article/details/80719302`

